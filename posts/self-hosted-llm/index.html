<!doctype html><html class=dark lang=en><head><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><title>Exploring AI: Self Hosted LLM</title><meta content="Exploring AI: Self Hosted LLM" property=og:title><meta content="Because running your own AI is the real power move." property=og:description><meta content="Because running your own AI is the real power move." name=description><link href=https://svnscha.de/posts/self-hosted-llm/ rel=canonical><link href=/favicon.ico rel=icon type=image/png><link href=/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link href=/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/site.webmanifest rel=manifest><link href=https://svnscha.de/fonts.css rel=stylesheet><script>var _paq=window._paq=window._paq||[];_paq.push(['disableCookies']);_paq.push(['trackPageView']);_paq.push(['enableLinkTracking']);_paq.push(['enableHeartBeatTimer']);(function(){var a="//analytics.liasoft.de/";_paq.push(['setTrackerUrl',a+ 'matomo.php']);_paq.push(['setSiteId','27']);var b=document,c=b.createElement('script'),d=b.getElementsByTagName('script')[0];c.async=true;c.src=a+ 'matomo.js';d.parentNode.insertBefore(c,d)})()</script><link href=https://svnscha.de/atom.xml rel=alternate title=svnscha type=application/atom+xml><link href=https://svnscha.de/theme/dark.css rel=stylesheet><link href=https://svnscha.de/main.css media=screen rel=stylesheet><link href=https://svnscha.de/lightbox.css rel=stylesheet><body><div class=content><header><style>.avatar{vertical-align:middle;width:150px;height:150px;border-radius:50%}#wrapper{width:100%;clear:both;text-align:center}</style><div class=main><a href=https://svnscha.de title=svnscha>svnscha</a><div class=socials><a class=social href=https://www.linkedin.com/in/svnscha/ rel=me> <img alt=linkedin src=/social_icons/linkedin.svg> </a><a class=social href=https://github.com/svnscha/ rel=me> <img alt=github src=/social_icons/github.svg> </a></div></div><nav><a href=/about style=margin-left:.7em>/about</a><a href=/notes style=margin-left:.7em>/notes</a></nav><div id=wrapper><a href=https://svnscha.de title=svnscha> <img alt="svnscha - Profile Picture" class=avatar src=/svnscha.webp> </a></div></header><main><article><div class=title><h1 class=page-header>Exploring AI: Self Hosted LLM</h1><div class=meta>Posted on <time>2025-03-17</time></div></div><section class=body><h1 id=why-you-ask>Why, You Ask?</h1><p>So, I decided to self-host an LLM. Why? Every time you use an online AI model, you're handing over your data to some company. Whether it's casual conversations, coding snippets, or business-related queries, everything you type is potentially being logged, analyzed, or even used to train future models. No thanks.<p>Instead, I prefer to keep things local. Also, I have an RTX ADA 4000 with 20GB of memory sitting here, so why not put it to good use?<p>Enter <a href=https://ollama.com/>Ollama</a>, an absurdly flexible service that makes running LLMs locally a breeze. Combine that with <a href=https://github.com/open-webui/open-webui>Open WebUI</a>, which ties everything together into a neat little interface, and of course, my go-to Nginx reverse proxy for easy access.<p>Let's break down the setup.<h2 id=step-1-install-ollama>Step 1: Install Ollama</h2><p>Ollama makes deploying LLMs locally ridiculously simple. Here's how to install it:<pre class=language-bash data-lang=bash style=background:#282c34;color:#abb2bf><code class=language-bash data-lang=bash><span style=color:#e06c75>curl -fsSL</span><span> https://ollama.com/install.sh | </span><span style=color:#e06c75>sh
</span></code></pre><p>This will install Ollama and set up everything you need to start running models locally. Want to make sure it's working? Just run:<pre class=language-bash data-lang=bash style=background:#282c34;color:#abb2bf><code class=language-bash data-lang=bash><span style=color:#e06c75>ollama</span><span> run codellama:13b
</span></code></pre><p>If you see an interactive prompt, congrats - you've got a local LLM running!<h2 id=step-2-install-open-webui>Step 2: Install Open WebUI</h2><p>Ollama is great, but a web interface makes it even better. That's where Open WebUI comes in. It gives you a sleek, chat-like interface to interact with your models.<p>To install Open WebUI manually without Docker, follow these steps:<h3 id=1-create-a-virtual-environment>1. Create a Virtual Environment</h3><pre class=language-bash data-lang=bash style=background:#282c34;color:#abb2bf><code class=language-bash data-lang=bash><span style=color:#e06c75>python3 -m</span><span> venv </span><span style=color:#e06c75>~</span><span>/openwebui-venv
</span><span style=color:#56b6c2>source </span><span style=color:#e06c75>~</span><span>/openwebui-venv/bin/activate
</span></code></pre><h3 id=2-install-open-webui>2. Install Open WebUI</h3><pre class=language-bash data-lang=bash style=background:#282c34;color:#abb2bf><code class=language-bash data-lang=bash><span style=color:#e06c75>pip</span><span> install open-webui
</span></code></pre><h3 id=3-create-a-systemd-service>3. Create a Systemd Service</h3><p>To make sure Open WebUI runs on startup, create a systemd service file:<pre class=language-bash data-lang=bash style=background:#282c34;color:#abb2bf><code class=language-bash data-lang=bash><span style=color:#e06c75>sudo</span><span> nano /etc/systemd/system/openwebui.service
</span></code></pre><p>Paste the following content:<pre class=language-bash data-lang=bash style=background:#282c34;color:#abb2bf><code class=language-bash data-lang=bash><span style=color:#e06c75>[Unit]
</span><span style=color:#e06c75>Description</span><span>=</span><span style=color:#98c379>Open </span><span style=color:#e06c75>WebUI</span><span> Service
</span><span style=color:#e06c75>After</span><span>=</span><span style=color:#98c379>network.target
</span><span>
</span><span style=color:#e06c75>[Service]
</span><span style=color:#e06c75>User</span><span>=</span><span style=color:#98c379>$</span><span style=color:#e06c75>USER
</span><span style=color:#e06c75>Group</span><span>=</span><span style=color:#98c379>$</span><span style=color:#e06c75>USER
</span><span style=color:#e06c75>WorkingDirectory</span><span>=</span><span style=color:#98c379>/home/$</span><span style=color:#e06c75>USER</span><span style=color:#98c379>/openwebui-venv
</span><span style=color:#e06c75>ExecStart</span><span>=</span><span style=color:#98c379>/home/$</span><span style=color:#e06c75>USER</span><span style=color:#98c379>/openwebui-venv/bin/open-webui
</span><span style=color:#e06c75>Restart</span><span>=</span><span style=color:#98c379>always
</span><span>
</span><span style=color:#e06c75>[Install]
</span><span style=color:#e06c75>WantedBy</span><span>=</span><span style=color:#98c379>multi-user.target
</span></code></pre><p>Save and exit, then reload systemd and enable the service:<pre class=language-bash data-lang=bash style=background:#282c34;color:#abb2bf><code class=language-bash data-lang=bash><span style=color:#e06c75>sudo</span><span> systemctl daemon-reload
</span><span style=color:#e06c75>sudo</span><span> systemctl enable openwebui.service
</span><span style=color:#e06c75>sudo</span><span> systemctl start openwebui.service
</span></code></pre><h2 id=step-3-reverse-proxy-with-nginx>Step 3: Reverse Proxy with Nginx</h2><p>Now, let's make accessing our LLM easier by setting up an Nginx reverse proxy. This way, we can reach Open WebUI without exposing it directly.<p>Here's a basic Nginx config:<pre style=background:#282c34;color:#abb2bf><code><span>server {
</span><span>    listen 443 ssl;
</span><span>    server_name chat.example.com;
</span><span>
</span><span>    location / {
</span><span>        proxy_pass http://localhost:8080;
</span><span>        proxy_set_header Host $host;
</span><span>        proxy_set_header X-Real-IP $remote_addr;
</span><span>        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
</span><span>    }
</span><span>}
</span></code></pre><p>Reload Nginx with:<pre class=language-bash data-lang=bash style=background:#282c34;color:#abb2bf><code class=language-bash data-lang=bash><span style=color:#e06c75>sudo</span><span> systemctl restart nginx
</span></code></pre><p>Now, you can access your self-hosted LLM via <code>https://chat.example.com</code>. Fancy.<h2 id=you-own-your-data-now>You Own Your Data Now</h2><p>One of the biggest advantages of self-hosting an LLM? Your data stays with you.<p>No sending queries to an external API, no third-party tracking what you're asking, no potential leaks of sensitive information. It's all running on your hardware, fully under your control. Whether you're experimenting with code, processing confidential documents, or just having fun chatting with AI, everything stays local.<h2 id=model-sizes-performance>Model Sizes & Performance</h2><p>Of course, different models come with different memory requirements. Here's what I'm running on my RTX ADA 4000 and how much VRAM they use:<pre style=background:#282c34;color:#abb2bf><code><span>NAME             ID              SIZE      PROCESSOR    UNTIL              
</span><span>codellama:7b     8fdf8f752f6e    9.4 GB    100% GPU     2 minutes from now    
</span><span>codellama:13b    9f438cb9cd58    15 GB     100% GPU     4 minutes from now    
</span><span>gemma3:12b       6fd036cefda5    13 GB     100% GPU     4 minutes from now    
</span></code></pre><p>This means I can comfortably run mid-sized models like <code>codellama:13b</code> while keeping things snappy.<h2 id=choosing-the-right-gpu>Choosing the Right GPU</h2><p>Picking the right GPU is all about balancing performance, VRAM, and cost - because, let's be honest, unless you're running an AI research lab, you're not dropping $30,000 on an H100.<p>Here's a quick breakdown of solid options:<ul><li>NVIDIA RTX 3090 - Powerful, with 24 GB of VRAM, but it's last-gen. ($$)<li>NVIDIA RTX 4090 - Even more powerful, with 24 GB of VRAM and better efficiency. ($$$)<li>NVIDIA RTX 5090 - Even more and more powerful, with 24 GB of VRAM and better efficiency. ($$$$)<li>NVIDIA RTX 4000 ADA Generation - Less powerful, but 20 GB of VRAM, and a low-profile card. ($)</ul><p>Now, why the ADA 4000? While the RTX 5090 is the fastest in raw compute power, VRAM is king for training AI models. The ADA 4000's 20 GB VRAM gives you enough room for Stable Diffusion training, larger batch sizes, and AI experiments, without hitting the limits of other consumer GPUs such as 3080 (10 GB).<p>Performance-wise, the 4090 and 5090 has more horsepower, but for training workloads where memory matters more than raw speed, the ADA 4000 is the more practical and cost-efficient choice. Plus, lower power consumption makes it a better long-term option if you're running AI workloads frequently. Also, the physical size - I mean it's a low profile card. Small, fits perfectly into any case.<p>At the end of the day, if you're serious about AI training and need a balance of VRAM, price, and efficiency, the ADA 4000 is the way to get started.<h2 id=example>Example</h2><h3 id=asking-codellama-13b>Asking codellama:13b</h3><p>Prompt: <code>implement fibonacci in python and also some unit tests using pytest</code></p><video class=cast controls src=/casts/codellama-13b-fibonacci.webm>Your browser does not support the video tag.</video><h3 id=asking-gemma3-12b>Asking gemma3:12b</h3><p>Prompt: <code>implement fibonacci in python and also some unit tests using pytest</code></p><video class=cast controls src=/casts/gemma3-12b-fibonacci.webm>Your browser does not support the video tag.</video><h2 id=wrapping-up>Wrapping Up</h2><p>With an RTX ADA 4000, Ollama, Open WebUI, and an Nginx reverse proxy, I now have an AI-powered assistant running entirely on my own hardware. No subscriptions, no cloud dependencies, just raw, local AI power. If you're serious about AI and privacy, setting this up is a no-brainer. Give it a try, and let your GPU do some work.</section></article></main><hr><center><small>Copyright Â© 2025 Sven Scharmentke. All rights reserved.</small></center></div><script src=https://svnscha.de/lightbox.js></script>